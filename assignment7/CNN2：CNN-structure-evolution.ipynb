{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN2：CNN-structure-evolution\n",
    "1986 Hinton 提出 BP <br>\n",
    "1989 LeCun 应用BP在CNN中<br>\n",
    "1998 LeNet 现代神经网络<br>\n",
    "     Hinton 发明 Dropout ReLU<br>\n",
    "2012 AlexNet<br>\n",
    "2015 ResNet<br>\n",
    "LeNet-5(1998) -> AlexNet(2012) -> ZF(2013) -> VGG(2014) -> GoogleNet(2014) -> ResNet(2015) -> GoogleNet-v4(2016)\n",
    "![Network](./img/1.jpg)\n",
    "![](./img/2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.LeNet-5， AlexNet\n",
    "![LeNet](./img/leNet-5.jpg)\n",
    "LeNet-5(1998): CNN鼻祖，定义了CNN的基本组建，用于手写数字识别，conv5x5 -> maxpool -> conv5x5 -> maxpool -> fc ->\n",
    "![AlexNet](./img/AlexNet.jpg)\n",
    "AlexNet(2012):\n",
    "- 使用数据增广增加模型泛化能力\n",
    "- 用ReLU代替Sigmoid来增加SGD的收敛速度\n",
    "- 提出Dropout防止过拟合\n",
    "- 使用多GPU进行训练：将上层的feature map按照通道维度拆分为2份，分别送入2个GPU，最早出现Group Convolution\n",
    "- LRN(Local Responce Normalization)：局部响应归一层，对W*H*C的特征图图的每一个点的所有通道1*1*C做归一化，后来研究者发现，并没有什么软用。\n",
    "### 2.VGG（2014）\n",
    "网络开始越来越深\n",
    "![VGG](./img/vgg.png)\n",
    "\n",
    "- 探索了CNN的深度与性能的关系，证明了网络深度在一定程度上影响网络最终的性能，上表描述了VggNet的网络结构以及诞生过程。为了解决权重初始化等问题，采用一种Pre-training的方式，先训练一部分小网络，确保这部分网络稳定之后，再逐渐加深。当网络处于D阶段的时候，效果最优，VGG-16表示conv+fc的总层数是16，不包括max pool\n",
    "- 使用小卷积核，3x3卷积核的优点：（1）两个3x3卷积核的感受野和一个5x5卷积核的感受野相同(3个3x3 filter与1个7x7感受野相同），但具有更少的参数，（2）引入了更多的非线性，增强模型的拟合能力\n",
    "\n",
    "### 3.GoogLeNet（2014）\n",
    "（1）参数更少，GoogleNet参数为500万个，AlexNet参数个数是GoogleNet的12倍，VGGNet参数又是AlexNet的3倍\n",
    "（2）性能更好：占用更少的内存和计算资源，且模型结果的性能却更加优越。  \n",
    "（3）Inception-v4引入了shortcut connection\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79ly1fmprhdocouj30qb08vac3.jpg)\n",
    "Inception blobk:\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79gy1fmprivb2hxj30dn09dwef.jpg)\n",
    "- 引入Inception block，使用1x1卷积核降维，降低计算量\n",
    "- 中间层引入Loss单元\n",
    "- 最后全连接层替换为global average pooling,减少参数量\n",
    "\n",
    "### 4.ResNet(2015)\n",
    "通过实验可以发现：随着网络层级的不断增加，模型精度不断得到提升，而当网络层级增加到一定的数目以后，训练精度和测试精度迅速下降，这说明当网络变得很深以后，深度网络就变得更加难以训练了\n",
    "![](./img/resnet1.png)\n",
    "深层模型难以训练的原因：  \n",
    "神经网络在反向传播过程中要不断地传播梯度，而当网络层数加深时，梯度在传播过程中会逐渐消失（假如采用Sigmoid函数，对于幅度为1的信号，每向后传递一层，梯度就衰减为原来的0.25，层数越多，衰减越厉害），导致无法对前面网络层的权重进行有效的调整。\n",
    "ResNet Block:\n",
    "![](./img/resnet2.jpg)\n",
    "![](./img/resnet3.jpg)\n",
    "- 引入shortcut connection解决了梯度回传消失的问题。\n",
    "- 层数非常深"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.DenseNet(2017)\n",
    "<img src=\"https://ws1.sinaimg.cn/large/006tNc79ly1fmpvj7fxd1j30vb0eyzqf.jpg\" \n",
    "- 密集连接：缓解梯度消失问题，加强特征传播，鼓励特征复用，极大的减少了参数量\n",
    "- 层数非常深非常深\n",
    "- 效果更好\n",
    "- 及其恐怖的内存占用\n",
    "\n",
    "#### FLOPs\n",
    "FLOPs:floating-point operations 浮点运算次数\n",
    "FLOPS:floating-point operations per second 每秒浮点数运算次数/每秒峰值速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
